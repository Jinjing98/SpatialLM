### model
model_name_or_path: manycore-research/SpatialLM1.1-Qwen-0.5B
trust_remote_code: true

### 3D Positional Encoding Configuration (uncomment to enable)
# pcd_theta: 10000             # Base frequency for 3D PE (default: 10000)
# disable_flash_attn: false    # Whether to disable flash attention (default: false)
VLM_PE: "CCA_2DProj"         # Options: null (comment out; default 1D RoPE), "CCA_2DProj", "3D_RoPE", "3D_Sinusoidal"

### CCA (Concentric Causal Attention) Configuration
cca_configs:
  grid_size: 24              # 2D grid size for point cloud projection (e.g., 24x24 = 576 cells)
  projection: "top_down"     # Orthographic projection: "top_down" (XY), "front" (XZ), "side" (YZ)
  pcd_norm_method: "gridsizeNorm"  # Point cloud normalization: "adaptiveNorm" (min-max of current PC), "gridsizeNorm" (based on grid size)

# media_dir: /data/horse/ws/jixu233b-metadata_ws/datasets/arkitscenes-spatiallm # arkitscenes-spatiallm

### method
do_train: true
freeze_language_tower: false
freeze_point_tower: false
train_proj_only: false

### dataset
dataset: arkitscenes_train # arkitscenes_train,arkitscenes_val
dataset_dir: /data/horse/ws/jixu233b-metadata_ws/datasets/arkitscenes-spatiallm 
template: spatiallm_qwen # spatiallm_qwen, spatiallm_llama
num_bins: 1280
do_augmentation: false
random_rotation: true
cutoff_len: 8192
overwrite_cache: true
preprocessing_num_workers: 8
dataloader_num_workers: 8
# used for index the data (will use the history ones)
save_dir: /data/horse/ws/jixu233b-metadata_ws/exps/train/saves/arkitscenes # save_dir: saves/arkitscenes
### output
# output_dir: /mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/saves
# Note: Timestamp auto-appended (e.g., spatiallm_0126_153045). Disable: AUTO_TIMESTAMP_OUTPUT_DIR=0
output_dir: /data/horse/ws/jixu233b-metadata_ws/exps/train/spatiallm
logging_steps: 10
save_steps: 500
overwrite_output_dir: false
save_total_limit: 5

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 1.0e-5 # to be more aggressive: try to change the learning rate to 5.0e-5 or 1.0e-4
num_train_epochs: 10
lr_scheduler_type: cosine
warmup_ratio: 0.03
pure_bf16: false
bf16: false
fp16: false
tf32: true
ddp_timeout: 180000000
report_to: wandb # choices: [none, wandb, tensorboard, swanlab]
max_new_tokens: 4096

### eval
val_size: 0.0001
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
