# MixedRoPE3D: æ ¸å¿ƒè®¾è®¡ä¸Žå®žçŽ°é™·é˜±

## æ¦‚è¿°

MixedRoPE3D æ˜¯ä¸€ä¸ªä¸º Point Cloud VLM (Vision-Language Model) è®¾è®¡çš„ä½ç½®ç¼–ç æ–¹æ¡ˆ,èƒ½å¤ŸåŒæ—¶å¤„ç†:
- **3D ç©ºé—´ä½ç½®ç¼–ç ** (åŸºäºŽ x, y, z åæ ‡) ç”¨äºŽ point cloud tokens
- **1D æ—¶åºä½ç½®ç¼–ç ** (åŸºäºŽ token é¡ºåº) ç”¨äºŽ text tokens å’Œ point tokens çš„æ—¶åºä¿¡æ¯

---

## æ ¸å¿ƒè®¾è®¡

### 1. æ•´ä½“æž¶æž„

```
SpatialLMQwenForCausalLMMixedRoPE3D (æœ€å¤–å±‚ - VLM æ¨¡åž‹)
  â”œâ”€ forward_point_cloud()
  â”‚   â””â”€ æå– point features + grid coordinates
  â”‚
  â”œâ”€ forward()
  â”‚   â”œâ”€ æ’å…¥ point tokens åˆ° text embeddings
  â”‚   â”œâ”€ åˆ›å»º point_token_mask [B, seq_len] (æ ‡è®°å“ªäº›æ˜¯ point tokens)
  â”‚   â””â”€ æ”¶é›† point_coords_list [N_tokens, 3] (grid coordinates)
  â”‚
  â””â”€ Qwen2ModelMixedRoPE3D (Transformer æ¨¡åž‹)
      â””â”€ MixedRoPE3DQwen2DecoderLayer Ã— 24 layers
          â””â”€ MixedRoPE3DQwen2Attention
              â””â”€ _apply_mixed_rope()
                  â”œâ”€ Point tokens: 3D RoPE (spatial) + 1D RoPE (temporal)
                  â””â”€ Text tokens: 1D RoPE (full head_dim)
```

### 2. ç©ºé—´-æ—¶åºåˆ†ç¦»ç­–ç•¥ (Spatial-Temporal Separation)

**æ ¸å¿ƒæ€æƒ³**: Point tokens åŒ…å« 4 ç§ä½ç½®ä¿¡æ¯:
- `temporal_order`: åœ¨æ•´ä¸ªåºåˆ—ä¸­çš„ä½ç½®
- `(x, y, z)`: 3D ç©ºé—´åæ ‡

ä¸ºäº†åŒæ—¶ç¼–ç è¿™ä¸¤ç§ä¿¡æ¯,`head_dim` è¢«åˆ†æˆä¸¤éƒ¨åˆ†:

```python
# é»˜è®¤ç­–ç•¥: 'half_spatial_half_temp'
head_dim = 32  # Qwen2-7B

spatial_dim = head_dim // 2 = 16   # ä¸ŠåŠéƒ¨åˆ†: ç”¨äºŽ 3D RoPE
temporal_dim = head_dim // 2 = 16  # ä¸‹åŠéƒ¨åˆ†: ç”¨äºŽ 1D RoPE
```

**åº”ç”¨æ–¹å¼**:

```python
# Point tokens
q_point = query_states[:, :, point_indices, :]  # [B, num_heads, N_point, head_dim]

# åˆ†å‰² head_dim
q_point_spatial = q_point[:, :, :, :spatial_dim]     # [B, num_heads, N_point, 16]
q_point_temporal = q_point[:, :, :, spatial_dim:]    # [B, num_heads, N_point, 16]

# åˆ†åˆ«åº”ç”¨ RoPE
q_point_spatial_rotated = apply_rotary_emb_3d(q_point_spatial, freqs_cis_3d, point_coords)
q_point_temporal_rotated = apply_rotary_emb_1d(q_point_temporal, position_embeddings)

# åˆå¹¶
q_point_rotated = torch.cat([q_point_spatial_rotated, q_point_temporal_rotated], dim=-1)

# Text tokens: å…¨éƒ¨ä½¿ç”¨ 1D RoPE
q_text = query_states[:, :, text_indices, :]  # [B, num_heads, N_text, 32]
q_text_rotated = apply_rotary_emb_1d(q_text, position_embeddings)
```

**è®¾è®¡ç†ç”±**:
- âœ… åŒæ—¶ä¿ç•™ç©ºé—´å’Œæ—¶åºä¿¡æ¯
- âœ… é¿å…ä¿¡æ¯å†²çª
- âœ… å¯ä»¥ç‹¬ç«‹ä¼˜åŒ–ä¸¤ç§ä½ç½®ç¼–ç 
- âš ï¸ TODO: æ”¯æŒå…¶ä»–åˆ†é…ç­–ç•¥ (e.g., `full_spatial`, custom ratios)

### 3. Learned Frequencies with Axial Mixing

**ä¸‰å±‚å¯å­¦ä¹ æ€§**:

1. **Per-axis frequencies**: æ¯ä¸ªè½´ (x, y, z) æœ‰ç‹¬ç«‹çš„é¢‘çŽ‡
2. **Per-head specialization**: æ¯ä¸ª attention head æœ‰ä¸åŒçš„é¢‘çŽ‡
3. **Per-frequency-bin**: æ¯ä¸ªé¢‘çŽ‡ bin æœ‰ç‹¬ç«‹çš„å‚æ•°

```python
# Base frequencies: [3 axes, num_heads, dim//2 freq_bins]
freqs_base_3d = torch.stack([
    1.0 / (theta ** (torch.arange(0, spatial_dim, 2) / spatial_dim))
    for _ in range(3)  # x, y, z
])  # [3, spatial_dim//2]

# Learned per-axis, per-head frequencies
freqs_3d = nn.Parameter(
    freqs_base_3d.unsqueeze(1).expand(3, num_heads, -1) 
    + torch.randn(3, num_heads, spatial_dim//2) * 0.01
)  # [3, 14, 16] for Query (14 heads)

# Learned axial mixing weights: [num_heads, dim//2, 3]
axial_weights = nn.Parameter(
    torch.ones(num_heads, spatial_dim//2, 3) 
    + torch.randn(num_heads, spatial_dim//2, 3) * 0.01
)  # [14, 16, 3]
```

**è®¡ç®—æ··åˆé¢‘çŽ‡**:

```python
def compute_mixed_cis_3d(freqs_3d, point_coords, axial_weights):
    """
    Args:
        freqs_3d: [3, num_heads, dim//2] - learned frequencies
        point_coords: [N_tokens, 3] - (x, y, z) coordinates
        axial_weights: [num_heads, dim//2, 3] - mixing weights
    
    Returns:
        freqs_cis: [num_heads, N_tokens, dim//2] - complex frequencies
    """
    # Compute per-axis angle contributions
    freqs_x = freqs_3d[0] * point_coords[:, 0:1, None]  # [N, 1, 1] * [num_heads, dim//2]
    freqs_y = freqs_3d[1] * point_coords[:, 1:2, None]  # â†’ [num_heads, N, dim//2]
    freqs_z = freqs_3d[2] * point_coords[:, 2:3, None]
    
    # Apply learned mixing weights
    w_x = axial_weights[:, :, 0].unsqueeze(1)  # [num_heads, 1, dim//2]
    w_y = axial_weights[:, :, 1].unsqueeze(1)
    w_z = axial_weights[:, :, 2].unsqueeze(1)
    
    # Weighted combination
    freqs_combined = w_x * freqs_x + w_y * freqs_y + w_z * freqs_z  # [num_heads, N, dim//2]
    
    # Convert to complex exponentials
    freqs_cis = torch.polar(torch.ones_like(freqs_combined), freqs_combined)
    return freqs_cis
```

### 4. GQA (Grouped Query Attention) é€‚é…

**æŒ‘æˆ˜**: Qwen2 ä½¿ç”¨ GQA
- `num_attention_heads` = 14 (Query heads)
- `num_key_value_heads` = 2 (Key/Value heads)

**è§£å†³æ–¹æ¡ˆ**: åˆ†åˆ«ä¸º Q å’Œ KV åˆå§‹åŒ– learned parameters

```python
class MixedRoPE3DQwen2Attention:
    def __init__(self, config, layer_idx):
        super().__init__(config, layer_idx)
        
        self.num_heads = config.num_attention_heads  # 14
        self.num_key_value_heads = config.num_key_value_heads  # 2
        
        # Query frequencies: [3, 14, 16]
        self.freqs_3d_q = nn.Parameter(
            freqs_base_3d.unsqueeze(1).expand(3, self.num_heads, -1) + ...
        )
        self.axial_weights_3d_q = nn.Parameter(
            torch.ones(self.num_heads, spatial_dim//2, 3) + ...
        )
        
        # Key/Value frequencies: [3, 2, 16]
        self.freqs_3d_kv = nn.Parameter(
            freqs_base_3d.unsqueeze(1).expand(3, self.num_key_value_heads, -1) + ...
        )
        self.axial_weights_3d_kv = nn.Parameter(
            torch.ones(self.num_key_value_heads, spatial_dim//2, 3) + ...
        )
    
    def _apply_mixed_rope(self, query_states, key_states, ...):
        # Compute separate freqs for Q and KV
        freqs_cis_3d_q = compute_mixed_cis_3d(
            self.freqs_3d_q, point_coords, self.axial_weights_3d_q
        )  # [14, N, 16]
        
        freqs_cis_3d_kv = compute_mixed_cis_3d(
            self.freqs_3d_kv, point_coords, self.axial_weights_3d_kv
        )  # [2, N, 16]
        
        # Apply separately
        q_rotated = apply_rotary_emb_3d(q_spatial, freqs_cis_3d_q)
        k_rotated = apply_rotary_emb_3d(k_spatial, freqs_cis_3d_kv)
```

### 5. Grid Coordinates for Sparse Encoders

**é—®é¢˜**: Sparse point cloud encoders åŽ‹ç¼©ç‚¹äº‘
- Input: 106,328 ä¸ªåŽŸå§‹ç‚¹ `[N_raw, 3]`
- Output: 556 ä¸ª encoded tokens `[N_tokens, D]`

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ voxel grid çš„ä¸­å¿ƒåæ ‡

```python
def forward_point_cloud(self, point_cloud, device, dtype):
    if self.config.point_backbone_type == PointBackboneType.SCENESCRIPT:
        # Sparse voxel processing
        sparse_output = self.point_backbone.sparse_resnet(pc_sparse_tensor)
        sparse_list = sparse_uncollate(sparse_output)
        
        # Extract grid coordinates: [N_tokens, 3]
        grid_coords = sparse_list[0].C.float()
        
        # Normalize to [0, 1]
        grid_coords = grid_coords / (self.point_backbone.reduced_grid_size - 1)
        
        # Get encoded features
        encoded_features = self.point_backbone.input_proj(...)
        
        return self.point_proj(encoded_features), grid_coords
    
    elif self.config.point_backbone_type == PointBackboneType.SONATA:
        point = self.point_backbone.enc(point)
        
        # Extract grid coordinates
        grid_coords = point["grid_coord"].float()  # [N_tokens, 3]
        
        context = point["sparse_conv_feat"].features
        return self.point_proj(context), grid_coords
```

---

## å…³é”®é™·é˜±ä¸Žè§£å†³æ–¹æ¡ˆ

### ðŸ”´ é™·é˜± 1: KV Cache ä¸Ž Attention Mask ç»´åº¦ä¸åŒ¹é…

**é—®é¢˜æè¿°**:

åœ¨ autoregressive generation ä¸­å­˜åœ¨ä¸¤ä¸ªé˜¶æ®µ:

1. **Prefill é˜¶æ®µ** (ç¬¬ä¸€æ¬¡ forward):
   ```
   input_ids: [1, 231] text tokens
   â†’ æ’å…¥ point tokens
   inputs_embeds: [1, 786] (231 - 2 + 556 + 1)
   attention_mask: [1, 786] âœ“
   ```

2. **Generation é˜¶æ®µ** (åŽç»­æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª token):
   ```
   input_ids: [1, 1] (æ–°ç”Ÿæˆçš„ token)
   attention_mask: [1, 232] (transformers åº“è‡ªåŠ¨æ›´æ–°,åªè®°å½• text tokens!)
   past_key_values: 786 tokens (åŒ…å« point tokens)
   
   ç»“æžœ:
   attn_weights: [1, 14, 1, 787] (1 query Ã— 787 keys)
   attention_mask: [1, 1, 1, 232] 
   
   â†’ RuntimeError: size mismatch 787 vs 232
   ```

**æ ¹æœ¬åŽŸå› **: 
transformers åº“çš„ `prepare_inputs_for_generation` æ–¹æ³•ä¸çŸ¥é“æˆ‘ä»¬åœ¨ prefill é˜¶æ®µæ’å…¥äº†é¢å¤–çš„ point tokens,æ‰€ä»¥ `attention_mask` åªè¿½è¸ªåŽŸå§‹çš„ text tokensã€‚

**è§£å†³æ–¹æ¡ˆ**: åœ¨ `_prepare_decoder_attention_mask` ä¸­æ£€æµ‹å¹¶ä¿®å¤

```python
def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values):
    batch_size, seq_length = input_shape
    
    # Get past sequence length from KV cache
    past_key_values_length = 0
    if past_key_values is not None:
        if hasattr(past_key_values, 'get_seq_length'):
            past_key_values_length = past_key_values.get_seq_length()
        elif isinstance(past_key_values, (list, tuple)):
            past_key_values_length = past_key_values[0][0].shape[2]
    
    # Fix length mismatch during generation
    if attention_mask is not None and past_key_values_length > 0:
        expected_length = past_key_values_length + seq_length  # 786 + 1 = 787
        actual_length = attention_mask.shape[1]  # 232
        
        if actual_length < expected_length:
            # Pad with 1s (can attend to point tokens)
            padding_length = expected_length - actual_length  # 555
            attention_mask = torch.nn.functional.pad(
                attention_mask, 
                (0, padding_length), 
                value=1
            )
            # Now: [1, 787] âœ“
    
    # Continue with mask expansion...
```

**å…³é”®è¦ç‚¹**:
- âœ… å¡«å……å€¼ä¸º 1 (å¯ä»¥ attend)
- âœ… å¡«å……åœ¨æœ«å°¾ (å‡è®¾ point tokens æ’å…¥åœ¨ä¸­é—´ä½ç½®,ä½† mask åªéœ€è¦é•¿åº¦åŒ¹é…)
- âš ï¸ è¿™ä¸ªå‡è®¾åœ¨ prefill é˜¶æ®µåˆ›å»ºçš„ attention_mask æ˜¯æ­£ç¡®çš„åŸºç¡€ä¸Šæˆç«‹

---

### ðŸ”´ é™·é˜± 2: Prefill vs Generation çš„ RoPE åº”ç”¨é€»è¾‘

**é—®é¢˜æè¿°**:

Generation é˜¶æ®µæ¯æ¬¡åªç”Ÿæˆ 1 ä¸ªæ–°çš„ text token,ä¸åº”è¯¥ä¼ é€’ `point_coords` å’Œ `point_token_mask`,å¦åˆ™ä¼šå°è¯•å¯¹æ–° token åº”ç”¨ 3D RoPEã€‚

**é”™è¯¯å®žçŽ°**:
```python
# âœ— æ¯æ¬¡éƒ½ä¼ é€’ point_coords
outputs = self.model(
    inputs_embeds=inputs_embeds,
    point_coords=point_coords_list,
    point_token_mask=point_token_mask,
)
```

**æ­£ç¡®å®žçŽ°**:
```python
# âœ“ æ ¹æ® past_key_values åˆ¤æ–­é˜¶æ®µ
if past_key_values is None:
    # Prefill: ä¼ é€’ point ç›¸å…³å‚æ•°
    model_point_coords = point_coords_list
    model_point_token_mask = point_token_mask
else:
    # Generation: ä¸ä¼ é€’,åªç”¨æ ‡å‡† 1D RoPE
    model_point_coords = None
    model_point_token_mask = None

outputs = self.model(
    inputs_embeds=inputs_embeds,
    past_key_values=past_key_values,
    point_coords=model_point_coords,
    point_token_mask=model_point_token_mask,
)
```

**å…³é”®è¦ç‚¹**:
- âœ… `past_key_values is None` â†’ Prefill é˜¶æ®µ
- âœ… `past_key_values is not None` â†’ Generation é˜¶æ®µ
- âœ… Generation é˜¶æ®µæ–° token åªç”¨æ ‡å‡† 1D RoPE

---

### ðŸ”´ é™·é˜± 3: GQA å¯¼è‡´çš„ RoPE ç»´åº¦é”™è¯¯

**é—®é¢˜æè¿°**:

```python
# âœ— é”™è¯¯: ç”¨ Query heads çš„ freqs åº”ç”¨åˆ° Key states
freqs_cis_3d = compute_mixed_cis_3d(
    freqs_3d,  # [3, 14, 16] - 14 query heads
    point_coords,
    axial_weights
)  # â†’ [14, N, 16]

# Apply to both Q and K
q_rotated = apply_rotary_emb_3d(query_states, freqs_cis_3d)  # âœ“ [B, 14, N, 16]
k_rotated = apply_rotary_emb_3d(key_states, freqs_cis_3d)    # âœ— [B, 2, N, 16]

# RuntimeError: shape [2, ...] vs [14, ...]
```

**è§£å†³æ–¹æ¡ˆ**: åˆ†åˆ«ä¸º Q å’Œ KV ç”Ÿæˆ frequencies

```python
# âœ“ æ­£ç¡®: åˆ†åˆ«å¤„ç†
freqs_cis_3d_q = compute_mixed_cis_3d(
    self.freqs_3d_q,  # [3, 14, 16]
    point_coords,
    self.axial_weights_3d_q
)  # [14, N, 16]

freqs_cis_3d_kv = compute_mixed_cis_3d(
    self.freqs_3d_kv,  # [3, 2, 16]
    point_coords,
    self.axial_weights_3d_kv
)  # [2, N, 16]

q_rotated = apply_rotary_emb_3d(query_states, freqs_cis_3d_q)   # âœ“ [B, 14, N, 16]
k_rotated = apply_rotary_emb_3d(key_states, freqs_cis_3d_kv)    # âœ“ [B, 2, N, 16]
```

**å…³é”®è¦ç‚¹**:
- âš ï¸ GQA ä¸­ `num_attention_heads` â‰  `num_key_value_heads`
- âœ… å¿…é¡»åˆ†åˆ«åˆå§‹åŒ– Q å’Œ KV çš„ learned parameters
- âœ… åœ¨ `_apply_mixed_rope` ä¸­åˆ†åˆ«åº”ç”¨

---

### ðŸ”´ é™·é˜± 4: Point Coordinates æ•°é‡ä¸Ž Token æ•°é‡ä¸åŒ¹é…

**é—®é¢˜æè¿°**:

```python
# âœ— é”™è¯¯: è¿”å›žåŽŸå§‹ç‚¹äº‘åæ ‡
def forward_point_cloud(self, point_cloud, device, dtype):
    encoded_features = self.point_backbone(point_cloud)  # [556, D]
    raw_coords = point_cloud[:, :3]  # [106328, 3]
    return encoded_features, raw_coords  # æ•°é‡ä¸åŒ¹é…!

# å¯¼è‡´é”™è¯¯:
# attn_weights: [1, 14, 556, 556]
# freqs_cis_3d: [14, 106328, 16]
# RuntimeError: size mismatch
```

**è§£å†³æ–¹æ¡ˆ**: ä»Ž sparse encoder æå– grid coordinates

```python
# âœ“ æ­£ç¡®: è¿”å›žç¼–ç åŽ token å¯¹åº”çš„åæ ‡
def forward_point_cloud(self, point_cloud, device, dtype):
    sparse_output = self.point_backbone.sparse_resnet(pc_sparse_tensor)
    sparse_list = sparse_uncollate(sparse_output)
    
    # Extract grid coordinates for encoded tokens
    grid_coords = sparse_list[0].C.float()  # [556, 3] âœ“
    
    encoded_features = self.point_backbone.input_proj(...)  # [556, D]
    
    return encoded_features, grid_coords
```

**å…³é”®è¦ç‚¹**:
- âœ… `grid_coords.shape[0]` å¿…é¡»ç­‰äºŽ `encoded_features.shape[0]`
- âœ… Grid coordinates è¡¨ç¤ºæ¯ä¸ª token å¯¹åº”çš„ voxel ä¸­å¿ƒ
- âš ï¸ éœ€è¦å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´ (e.g., [0, 1] æˆ– [-1, 1])

---

### ðŸ”´ é™·é˜± 5: çˆ¶ç±»æ–¹æ³•è°ƒç”¨å¯¼è‡´å¡æ­»

**é—®é¢˜æè¿°**:

```python
# âœ— å°è¯•è°ƒç”¨çˆ¶ç±»çš„ attention mask å¤„ç†æ–¹æ³•
if attention_mask is not None:
    attention_mask = super()._update_causal_mask(
        attention_mask, inputs_embeds, cache_position, 
        past_key_values, output_attentions
    )

# ç»“æžœ: ç¨‹åºå¡æ­»,æ²¡æœ‰ä»»ä½•è¾“å‡º
```

**åŽŸå› **:
- `super()` è¿”å›žçš„ä»£ç†å¯¹è±¡,æ–¹æ³•ç­¾åå¯èƒ½ä¸å…¼å®¹
- transformers ä¸åŒç‰ˆæœ¬çš„ API å˜åŒ–
- å¯èƒ½è§¦å‘äº†æ— é™é€’å½’æˆ–å…¶ä»–æœªçŸ¥è¡Œä¸º

**è§£å†³æ–¹æ¡ˆ**: å®Œå…¨è‡ªå·±å®žçŽ° mask å¤„ç†é€»è¾‘

```python
# âœ“ æ­£ç¡®: è‡ªå·±å®žçŽ°å®Œæ•´é€»è¾‘
def _prepare_decoder_attention_mask(self, attention_mask, input_shape, ...):
    # 1. Get past length
    # 2. Fix length mismatch
    # 3. Create causal mask
    # 4. Expand attention mask
    # 5. Combine masks
    return combined_attention_mask
```

**å…³é”®è¦ç‚¹**:
- âœ… ä¸ä¾èµ–çˆ¶ç±»çš„ mask å¤„ç†æ–¹æ³•
- âœ… å®Œå…¨æŽ§åˆ¶ mask çš„ç”Ÿæˆå’Œå¤„ç†æµç¨‹
- âš ï¸ éœ€è¦ä»”ç»†å®žçŽ° causal mask çš„é€»è¾‘

---

### ðŸ”´ é™·é˜± 6: Batch Dimension å¤„ç†ä¸ä¸€è‡´

**é—®é¢˜æè¿°**:

`point_coords` å¯èƒ½æœ‰ä¸åŒçš„ shape:
- Prefill å•æ ·æœ¬: `[N_tokens, 3]`
- Prefill å¤šæ ·æœ¬: `[B, N_tokens, 3]`

**è§£å†³æ–¹æ¡ˆ**: ç»Ÿä¸€å¤„ç†

```python
def _apply_mixed_rope(self, query_states, key_states, position_embeddings,
                     point_coords, point_token_mask):
    # Ensure 3D: [B, N_tokens, 3]
    if point_coords.dim() == 2:
        point_coords = point_coords.unsqueeze(0)
    
    # Extract point indices from mask
    point_indices = point_token_mask[0].nonzero(as_tuple=True)[0]
    
    # Get coordinates for this batch
    if point_coords.shape[0] == 1:
        curr_point_coords = point_coords[0, :, :]  # [N_point, 3]
    else:
        curr_point_coords = point_coords[0, :, :]  # TODO: handle batch
    
    # Now process...
```

---

## Hyperparameters

```python
# 3D RoPE Configuration
rope_theta_3d: float = 100.0
    # Base frequency for 3D RoPE
    # Lower values â†’ slower position decay
    # Recommended: 100 for point clouds

spatial_temporal_separate_strategy: str = 'half_spatial_half_temp'
    # How to split head_dim between spatial and temporal
    # Options: 'half_spatial_half_temp', 'full_spatial', 'full_temporal', ...

# Learned Frequencies
rope_mixed_3d: bool = True
    # Whether to use learned frequencies (vs fixed)

mixedRoPE_3d_learned_per_axis: bool = True
    # Whether each axis (x,y,z) has independent learned frequencies

mixedRoPE_3d_learned_axial_mixing_weight: bool = True
    # Whether the mixing weights between axes are learned
    # If False: fixed 1:1:1 mixing
    # If True: learned per-head, per-bin weights [num_heads, dim//2, 3]
```

---

## Testing & Debugging

### å…³é”® Debug Points

1. **Prefill é˜¶æ®µ**:
   ```python
   print(f'input_ids: {input_ids.shape}')
   print(f'inputs_embeds after insertion: {inputs_embeds.shape}')
   print(f'attention_mask: {attention_mask.shape}')
   print(f'point_token_mask: {point_token_mask.shape}, num True: {point_token_mask.sum()}')
   print(f'point_coords: {point_coords.shape}')
   ```

2. **Generation é˜¶æ®µ**:
   ```python
   print(f'past_key_values_length: {past_key_values_length}')
   print(f'attention_mask before fix: {attention_mask.shape}')
   print(f'attention_mask after fix: {attention_mask.shape}')
   print(f'attn_weights: {attn_weights.shape}')
   ```

3. **RoPE åº”ç”¨**:
   ```python
   print(f'freqs_cis_3d_q: {freqs_cis_3d_q.shape}')
   print(f'freqs_cis_3d_kv: {freqs_cis_3d_kv.shape}')
   print(f'q_point_spatial: {q_point_spatial.shape}')
   print(f'k_point_spatial: {k_point_spatial.shape}')
   ```

### å¸¸è§é”™è¯¯ä¿¡å·

| é”™è¯¯ | å¯èƒ½åŽŸå›  | æ£€æŸ¥ç‚¹ |
|------|----------|--------|
| `RuntimeError: size mismatch ... 787 vs 232` | KV cache attention mask ä¸åŒ¹é… | `_prepare_decoder_attention_mask` |
| `RuntimeError: size mismatch ... 14 vs 2` | GQA freqs æœªåˆ†ç¦» | `freqs_3d_q` vs `freqs_3d_kv` |
| `RuntimeError: size mismatch ... 556 vs 106328` | Point coords æ•°é‡é”™è¯¯ | `forward_point_cloud` è¿”å›žå€¼ |
| ç¨‹åºå¡æ­»æ— è¾“å‡º | çˆ¶ç±»æ–¹æ³•è°ƒç”¨é—®é¢˜ | ç§»é™¤ `super()._update_causal_mask` |
| `AssertionError: shape mismatch` | Batch dimension ä¸ä¸€è‡´ | `point_coords.dim()` æ£€æŸ¥ |

---

## Future Improvements

1. **æ›´å¤šåˆ†ç¦»ç­–ç•¥**:
   - [ ] `full_spatial`: å…¨éƒ¨ç”¨äºŽ 3D RoPE
   - [ ] `custom_ratio`: å¯é…ç½®æ¯”ä¾‹ (e.g., 70% spatial, 30% temporal)
   - [ ] `adaptive`: æ ¹æ® task åŠ¨æ€è°ƒæ•´

2. **æ›´å¥½çš„ Grid Coordinates**:
   - [ ] æ”¯æŒéžå‡åŒ€ voxel grid
   - [ ] è€ƒè™‘ local neighborhood ä¿¡æ¯
   - [ ] Multi-scale grid coordinates

3. **æ€§èƒ½ä¼˜åŒ–**:
   - [ ] ç¼“å­˜ `freqs_cis` è®¡ç®—ç»“æžœ
   - [ ] Flash Attention é›†æˆ
   - [ ] Mixed precision ä¼˜åŒ–

4. **Attention Mask æ”¹è¿›**:
   - [ ] æ›´æ™ºèƒ½çš„ point token position tracking
   - [ ] æ”¯æŒåŠ¨æ€æ’å…¥/åˆ é™¤ tokens
   - [ ] Per-token attention control

---

## References

- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)
- [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671)

---

## Changelog

- **2026-02-07**: Initial implementation
  - Basic 3D RoPE with spatial-temporal separation
  - GQA adaptation
  - KV cache fix for generation
  - Learned frequencies with axial mixing
